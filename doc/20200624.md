# On compiling <s>fast</s> speedily

First a hard question: if you want to work on improving the <s>performance</s> speed of a slow compiler, do you need to know a lot about slow compilers? Or should you know/learn a lot about fast compilers first, apart what you know from compilers?

If yes then how? Lately I've been reading a lot of posts about how slow Ruby is to compile. It takes a lot of work but the result is OK, so people should but out and go with it? See, I've almost always have been working in Delphi's Object Pascal (and Turbo Pascal before that) and I know I've been spoiled. Almost literally. You code, hit run, and in most cases you might even miss the compiler progress box before the debugger kicks in already. What's more, in working with a toy compiler of my own, I've come acress this seemingly hyper-important thing called _linking_. Yes, growing up on Pascal I've never had to pay any attention to the bit where you're compiled code gets magically converted in the output binary file.

One thing that helps is incremental compilation, where you don't re-compile where the source-code hasn't changed anyway (a trick thing in the C-world with the pre-compiler). Opposite to that — so to speak — is where you already have a debugger running, but have a fresh bit of compiled code put into the live process' memory when code was changed when stepping through (I don't seem to recall what it's called). Looks like two ways to avoid the hard work of compiling too much source code (again), but still 'after the fact' if you ask me.

Having a fast and dependable compiler really makes a number of things possible. Not only a fast edit-debug-cycle, but over the other side with the interpreted languages it's now commonplace to have a JIT compiler conjure up some binary and have a processor do stuff much smoother than the interpreter, even without too much of _super-deluxe_ optimizer steps inbetween. It's a spectrum really, and anything with its own byte-code (Java, .Net, Python) will forever get locked in the middle. In case you haven't figured this out by yourself by now: that's where WebAssembly and GraalVM come in as attempts to reach the 'full binary and optimized' end of the spectrum.

Anyway, let's get technical and get to what I'm wanting to propose. I've seen LLVM for a bit, and not much yet of other compilers, but I guess some may suffer this same phenomenon: if you treat the 'compiler as a problem' and start to solve it with polymorphism and other object-oriented trickery, you'll end up with a nice neat tree of base-classes and inheritance, but building the abstract syntax tree with all those objects actually builds a wreck of convoluted mess in memory. I've always been sceptical of the hidden extra memory that gets allocated just to make objects work, but with objects in there with the others, once you'll need to walk the tree, for example to save the AST to file, it'll just take too much time to walk over all those nodes, all over the place, with no change of a CPU having it stream into the L1 cache...

So, that's why for my own little toy compiler, I first allocate a good chunk of contiguous memory, and then exclusively work with relative pointers into that block. Once it's time to store that data to somewhere else, _bam_ just spool the block of data into a file. (Come to think of it, I might change that to a memory-mapped file to begin with at sometime in the future...)